{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZphW-RwmuoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj0iNcs4mAY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "6b6cb417-07c5-4683-c5cc-0666ccf98edc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xb_7xKRmAzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "c8746334-4113-4c24-ab4d-eda213073226"
      },
      "source": [
        "!pip install stop-words"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop-words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32917 sha256=1c9d490e63c5a261a3207d0423a7ab2705d95ffcb5fc5b15e0da944412b63a25\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9A-IhxmXqI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "d16bf558-6477-4e9d-ee49-7c30d80b689a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHEHx-7KmaCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "stop_words = get_stop_words('en')\n",
        "\n",
        "replacement = {\n",
        "    \"aren't\" : \"are not\",\n",
        "    \"can't\" : \"cannot\",\n",
        "    \"couldn't\" : \"could not\",\n",
        "    \"didn't\" : \"did not\",\n",
        "    \"doesn't\" : \"does not\",\n",
        "    \"don't\" : \"do not\",\n",
        "    \"hadn't\" : \"had not\",\n",
        "    \"hasn't\" : \"has not\",\n",
        "    \"haven't\" : \"have not\",\n",
        "    \"he'd\" : \"he would\",\n",
        "    \"he'll\" : \"he will\",\n",
        "    \"he's\" : \"he is\",\n",
        "    \"i'd\" : \"I would\",\n",
        "    \"i'll\" : \"I will\",\n",
        "    \"i'm\" : \"I am\",\n",
        "    \"isn't\" : \"is not\",\n",
        "    \"it's\" : \"it is\",\n",
        "    \"it'll\":\"it will\",\n",
        "    \"i've\" : \"I have\",\n",
        "    \"let's\" : \"let us\",\n",
        "    \"mightn't\" : \"might not\",\n",
        "    \"mustn't\" : \"must not\",\n",
        "    \"shan't\" : \"shall not\",\n",
        "    \"she'd\" : \"she would\",\n",
        "    \"she'll\" : \"she will\",\n",
        "    \"she's\" : \"she is\",\n",
        "    \"shouldn't\" : \"should not\",\n",
        "    \"that's\" : \"that is\",\n",
        "    \"there's\" : \"there is\",\n",
        "    \"they'd\" : \"they would\",\n",
        "    \"they'll\" : \"they will\",\n",
        "    \"they're\" : \"they are\",\n",
        "    \"they've\" : \"they have\",\n",
        "    \"we'd\" : \"we would\",\n",
        "    \"we're\" : \"we are\",\n",
        "    \"weren't\" : \"were not\",\n",
        "    \"we've\" : \"we have\",\n",
        "    \"what'll\" : \"what will\",\n",
        "    \"what're\" : \"what are\",\n",
        "    \"what's\" : \"what is\",\n",
        "    \"what've\" : \"what have\",\n",
        "    \"where's\" : \"where is\",\n",
        "    \"who'd\" : \"who would\",\n",
        "    \"who'll\" : \"who will\",\n",
        "    \"who're\" : \"who are\",\n",
        "    \"who's\" : \"who is\",\n",
        "    \"who've\" : \"who have\",\n",
        "    \"won't\" : \"will not\",\n",
        "    \"wouldn't\" : \"would not\",\n",
        "    \"you'd\" : \"you would\",\n",
        "    \"you'll\" : \"you will\",\n",
        "    \"you're\" : \"you are\",\n",
        "    \"you've\" : \"you have\",\n",
        "    \"'re\": \" are\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'll\":\" will\",\n",
        "    \"tryin'\":\"trying\",\n",
        "}\n",
        "\n",
        "def lower(sentence):\n",
        "      return sentence.lower()\n",
        "\n",
        "def replace(sentence):\n",
        "      # Replace words like gooood to good\n",
        "      sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
        "      # Normalize common abbreviations\n",
        "      words = sentence.split(' ')\n",
        "      words = [replacement[word] if word in replacement else word for word in words]\n",
        "      sentence_repl = \" \".join(words)\n",
        "      return sentence_repl\n",
        "\n",
        "def remove_website(sentence):\n",
        "      sentence_repl = sentence.replace(r\"http\\S+\", \"\")\n",
        "      sentence_repl = sentence_repl.replace(r\"https\\S+\", \"\")\n",
        "      sentence_repl = sentence_repl.replace(r\"http\", \"\")\n",
        "      sentence_repl = sentence_repl.replace(r\"https\", \"\")\n",
        "      return sentence_repl\n",
        "\n",
        "def remove_name_tag(sentence):\n",
        "      # Remove name tag\n",
        "      sentence_repl = sentence.replace(r\"@\\S+\", \"\")\n",
        "      return sentence_repl\n",
        "\n",
        "def remove_time(sentence):\n",
        "      # Remove time related text\n",
        "      sentence_repl = sentence.replace(r'\\w{3}[+-][0-9]{1,2}\\:[0-9]{2}\\b', \"\")  # e.g. UTC+09:00\n",
        "      sentence_repl = sentence_repl.replace(r'\\d{1,2}\\:\\d{2}\\:\\d{2}', \"\")  # e.g. 18:09:01\n",
        "      sentence_repl = sentence_repl.replace(r'\\d{1,2}\\:\\d{2}', \"\")  # e.g. 18:09\n",
        "      # Remove date related text\n",
        "      # e.g. 11/12/19, 11-1-19, 1.12.19, 11/12/2019\n",
        "      sentence_repl = sentence_repl.replace(r'\\d{1,2}(?:\\/|\\-|\\.)\\d{1,2}(?:\\/|\\-|\\.)\\d{2,4}', \"\")\n",
        "      # e.g. 11 dec, 2019   11 dec 2019   dec 11, 2019\n",
        "      sentence_repl = sentence_repl.replace(\n",
        "          r\"([\\d]{1,2}\\s(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)|(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\s[\\d]{1,2})(\\s|\\,|\\,\\s|\\s\\,)[\\d]{2,4}\",\n",
        "          \"\")\n",
        "      # e.g. 11 december, 2019   11 december 2019   december 11, 2019\n",
        "      sentence_repl = sentence_repl.replace(\n",
        "          r\"[\\d]{1,2}\\s(january|february|march|april|may|june|july|august|september|october|november|december)(\\s|\\,|\\,\\s|\\s\\,)[\\d]{2,4}\",\n",
        "            \"\")\n",
        "      return sentence_repl\n",
        "\n",
        "def remove_breaks(sentence):\n",
        "      # Remove line breaks\n",
        "      sentence_repl = sentence.replace(\"\\r\", \"\")\n",
        "      sentence_repl = sentence_repl.replace(\"\\n\", \"\")\n",
        "      sentence_repl = re.sub(r\"\\\\n\\n\", \".\", sentence_repl)\n",
        "      return sentence_repl\n",
        "\n",
        "def remove_ip(sentence):\n",
        "      # Remove phone number and IP address\n",
        "      sentence_repl = sentence.replace(r'\\d{8,}', \"\")\n",
        "      sentence_repl = sentence_repl.replace(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', \"\")\n",
        "      return sentence_repl\n",
        "\n",
        "def adjust_common(sentence):\n",
        "      # Adjust common abbreviation\n",
        "      sentence_repl = sentence.replace(r\" you re \", \" you are \")\n",
        "      sentence_repl = sentence_repl.replace(r\" we re \", \" we are \")\n",
        "      sentence_repl = sentence_repl.replace(r\" they re \", \" they are \")\n",
        "      sentence_repl = sentence_repl.replace(r\"@\", \"at\")\n",
        "      return sentence_repl\n",
        "\n",
        "def remove_stopword(sentence):\n",
        "      words = sentence.split()\n",
        "      x = [word for word in words if word not in stop_words]\n",
        "      return \" \".join(x)\n",
        "\n",
        "def call(x):\n",
        "      x = lower(x)\n",
        "      x = replace(x)\n",
        "      x = remove_website(x)\n",
        "      x = remove_name_tag(x)\n",
        "      x = remove_time(x)\n",
        "      x = remove_breaks(x)\n",
        "      x = remove_ip(x)\n",
        "      x = adjust_common(x)\n",
        "      x = remove_stopword(x)\n",
        "      return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuyNJul3meSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def nltk2wn_tag(nltk_tag):\n",
        "  if nltk_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif nltk_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif nltk_tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif nltk_tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:                    \n",
        "    return None\n",
        "    \n",
        "def lemmatize_sentence(sentence):\n",
        "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))    \n",
        "  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
        "  res_words = []\n",
        "  for word, tag in wn_tagged:\n",
        "    if tag is None:                        \n",
        "      res_words.append(word)\n",
        "    else:\n",
        "      res_words.append(lemmatizer.lemmatize(word, tag))\n",
        "  return \" \".join(res_words)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jku6gjqRmpA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply data-preprocessing to keywords in term\n",
        "df_train_term = pd.read_csv(\"/content/gdrive/My Drive/impact/train_term.csv\")\n",
        "for index, row in df_train_term.iterrows():\n",
        "  value = call(row['abstract'])\n",
        "  value = lemmatize_sentence(value)\n",
        "  df_train_term.loc[index,'abstract'] = value"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9uAY_oom97q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_val_term = pd.read_csv(\"/content/gdrive/My Drive/impact/val_term.csv\")\n",
        "for index, row in df_val_term.iterrows():\n",
        "  value = call(row['abstract'])\n",
        "  value = lemmatize_sentence(value)\n",
        "  df_val_term.loc[index,'abstract'] = value"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQMghlDumrLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test_term = pd.read_csv(\"/content/gdrive/My Drive/impact/test_term.csv\")\n",
        "for index, row in df_test_term.iterrows():\n",
        "  value = call(row['abstract'])\n",
        "  value = lemmatize_sentence(value)\n",
        "  df_test_term.loc[index,'abstract'] = value"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh8UdU7fm0Rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_term.to_csv('/content/gdrive/My Drive/impact/train_term_preprocessed.csv', index=False)\n",
        "df_val_term.to_csv('/content/gdrive/My Drive/impact/val_term_preprocessed.csv', index=False)\n",
        "df_test_term.to_csv('/content/gdrive/My Drive/impact/test_term_preprocessed.csv', index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGfL-KnNm2BI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply data-preprocessing to keywords in most-depth\n",
        "df_train_md = pd.read_csv(\"/content/gdrive/My Drive/impact/train_most_depth.csv\")\n",
        "for index, row in df_train_md.iterrows():\n",
        "  value = call(row['abstract'])\n",
        "  value = lemmatize_sentence(value)\n",
        "  df_train_md.loc[index,'abstract'] = value"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAN5dobDm4Kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_val_md = pd.read_csv(\"/content/gdrive/My Drive/impact/val_most_depth.csv\")\n",
        "for index, row in df_val_md.iterrows():\n",
        "  value = call(row['abstract'])\n",
        "  value = lemmatize_sentence(value)\n",
        "  df_val_md.loc[index,'abstract'] = value"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcph3EJam58L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test_md = pd.read_csv(\"/content/gdrive/My Drive/impact/test_most_depth.csv\")\n",
        "for index, row in df_test_md.iterrows():\n",
        "  value = call(row['abstract'])\n",
        "  value = lemmatize_sentence(value)\n",
        "  df_test_md.loc[index,'abstract'] = value"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQD39Enmm8Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_md.to_csv('/content/gdrive/My Drive/impact/train_md_preprocessed.csv', index=False)\n",
        "df_val_md.to_csv('/content/gdrive/My Drive/impact/val_md_preprocessed.csv', index=False)\n",
        "df_test_md.to_csv('/content/gdrive/My Drive/impact/test_md_preprocessed.csv', index=False)"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}