{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZphW-RwmuoA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "mj0iNcs4mAY0",
    "outputId": "6b6cb417-07c5-4683-c5cc-0666ccf98edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "7xb_7xKRmAzb",
    "outputId": "c8746334-4113-4c24-ab4d-eda213073226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop-words\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
      "Building wheels for collected packages: stop-words\n",
      "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32917 sha256=1c9d490e63c5a261a3207d0423a7ab2705d95ffcb5fc5b15e0da944412b63a25\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2018.7.23\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "colab_type": "code",
    "id": "bq9A-IhxmXqI",
    "outputId": "d16bf558-6477-4e9d-ee49-7c30d80b689a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHEHx-7KmaCT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words = get_stop_words('en')\n",
    "\n",
    "replacement = {\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"i'd\" : \"I would\",\n",
    "    \"i'll\" : \"I will\",\n",
    "    \"i'm\" : \"I am\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"i've\" : \"I have\",\n",
    "    \"let's\" : \"let us\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\",\n",
    "    \"shan't\" : \"shall not\",\n",
    "    \"she'd\" : \"she would\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"there's\" : \"there is\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"they've\" : \"they have\",\n",
    "    \"we'd\" : \"we would\",\n",
    "    \"we're\" : \"we are\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"we've\" : \"we have\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what've\" : \"what have\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who're\" : \"who are\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who've\" : \"who have\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"'re\": \" are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'll\":\" will\",\n",
    "    \"tryin'\":\"trying\",\n",
    "}\n",
    "\n",
    "def lower(sentence):\n",
    "      return sentence.lower()\n",
    "\n",
    "def replace(sentence):\n",
    "      # Replace words like gooood to good\n",
    "      sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "      # Normalize common abbreviations\n",
    "      words = sentence.split(' ')\n",
    "      words = [replacement[word] if word in replacement else word for word in words]\n",
    "      sentence_repl = \" \".join(words)\n",
    "      return sentence_repl\n",
    "\n",
    "def remove_website(sentence):\n",
    "      sentence_repl = sentence.replace(r\"http\\S+\", \"\")\n",
    "      sentence_repl = sentence_repl.replace(r\"https\\S+\", \"\")\n",
    "      sentence_repl = sentence_repl.replace(r\"http\", \"\")\n",
    "      sentence_repl = sentence_repl.replace(r\"https\", \"\")\n",
    "      return sentence_repl\n",
    "\n",
    "def remove_name_tag(sentence):\n",
    "      # Remove name tag\n",
    "      sentence_repl = sentence.replace(r\"@\\S+\", \"\")\n",
    "      return sentence_repl\n",
    "\n",
    "def remove_time(sentence):\n",
    "      # Remove time related text\n",
    "      sentence_repl = sentence.replace(r'\\w{3}[+-][0-9]{1,2}\\:[0-9]{2}\\b', \"\")  # e.g. UTC+09:00\n",
    "      sentence_repl = sentence_repl.replace(r'\\d{1,2}\\:\\d{2}\\:\\d{2}', \"\")  # e.g. 18:09:01\n",
    "      sentence_repl = sentence_repl.replace(r'\\d{1,2}\\:\\d{2}', \"\")  # e.g. 18:09\n",
    "      # Remove date related text\n",
    "      # e.g. 11/12/19, 11-1-19, 1.12.19, 11/12/2019\n",
    "      sentence_repl = sentence_repl.replace(r'\\d{1,2}(?:\\/|\\-|\\.)\\d{1,2}(?:\\/|\\-|\\.)\\d{2,4}', \"\")\n",
    "      # e.g. 11 dec, 2019   11 dec 2019   dec 11, 2019\n",
    "      sentence_repl = sentence_repl.replace(\n",
    "          r\"([\\d]{1,2}\\s(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)|(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\s[\\d]{1,2})(\\s|\\,|\\,\\s|\\s\\,)[\\d]{2,4}\",\n",
    "          \"\")\n",
    "      # e.g. 11 december, 2019   11 december 2019   december 11, 2019\n",
    "      sentence_repl = sentence_repl.replace(\n",
    "          r\"[\\d]{1,2}\\s(january|february|march|april|may|june|july|august|september|october|november|december)(\\s|\\,|\\,\\s|\\s\\,)[\\d]{2,4}\",\n",
    "            \"\")\n",
    "      return sentence_repl\n",
    "\n",
    "def remove_breaks(sentence):\n",
    "      # Remove line breaks\n",
    "      sentence_repl = sentence.replace(\"\\r\", \"\")\n",
    "      sentence_repl = sentence_repl.replace(\"\\n\", \"\")\n",
    "      sentence_repl = re.sub(r\"\\\\n\\n\", \".\", sentence_repl)\n",
    "      return sentence_repl\n",
    "\n",
    "def remove_ip(sentence):\n",
    "      # Remove phone number and IP address\n",
    "      sentence_repl = sentence.replace(r'\\d{8,}', \"\")\n",
    "      sentence_repl = sentence_repl.replace(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', \"\")\n",
    "      return sentence_repl\n",
    "\n",
    "def adjust_common(sentence):\n",
    "      # Adjust common abbreviation\n",
    "      sentence_repl = sentence.replace(r\" you re \", \" you are \")\n",
    "      sentence_repl = sentence_repl.replace(r\" we re \", \" we are \")\n",
    "      sentence_repl = sentence_repl.replace(r\" they re \", \" they are \")\n",
    "      sentence_repl = sentence_repl.replace(r\"@\", \"at\")\n",
    "      return sentence_repl\n",
    "\n",
    "def remove_stopword(sentence):\n",
    "      words = sentence.split()\n",
    "      x = [word for word in words if word not in stop_words]\n",
    "      return \" \".join(x)\n",
    "\n",
    "def call(x):\n",
    "      x = lower(x)\n",
    "      x = replace(x)\n",
    "      x = remove_website(x)\n",
    "      x = remove_name_tag(x)\n",
    "      x = remove_time(x)\n",
    "      x = remove_breaks(x)\n",
    "      x = remove_ip(x)\n",
    "      x = adjust_common(x)\n",
    "      x = remove_stopword(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuyNJul3meSc"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:                    \n",
    "    return None\n",
    "    \n",
    "def lemmatize_sentence(sentence):\n",
    "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))    \n",
    "  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "  res_words = []\n",
    "  for word, tag in wn_tagged:\n",
    "    if tag is None:                        \n",
    "      res_words.append(word)\n",
    "    else:\n",
    "      res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "  return \" \".join(res_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jku6gjqRmpA4"
   },
   "outputs": [],
   "source": [
    "# Apply data-preprocessing to keywords in term\n",
    "df_train_term = pd.read_csv(\"/content/gdrive/My Drive/impact/train_term.csv\")\n",
    "for index, row in df_train_term.iterrows():\n",
    "  value = call(row['abstract'])\n",
    "  value = lemmatize_sentence(value)\n",
    "  df_train_term.loc[index,'abstract'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9uAY_oom97q"
   },
   "outputs": [],
   "source": [
    "df_val_term = pd.read_csv(\"/content/gdrive/My Drive/impact/val_term.csv\")\n",
    "for index, row in df_val_term.iterrows():\n",
    "  value = call(row['abstract'])\n",
    "  value = lemmatize_sentence(value)\n",
    "  df_val_term.loc[index,'abstract'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQMghlDumrLq"
   },
   "outputs": [],
   "source": [
    "df_test_term = pd.read_csv(\"/content/gdrive/My Drive/impact/test_term.csv\")\n",
    "for index, row in df_test_term.iterrows():\n",
    "  value = call(row['abstract'])\n",
    "  value = lemmatize_sentence(value)\n",
    "  df_test_term.loc[index,'abstract'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jh8UdU7fm0Rf"
   },
   "outputs": [],
   "source": [
    "df_train_term.to_csv('/content/gdrive/My Drive/impact/train_term_preprocessed.csv', index=False)\n",
    "df_val_term.to_csv('/content/gdrive/My Drive/impact/val_term_preprocessed.csv', index=False)\n",
    "df_test_term.to_csv('/content/gdrive/My Drive/impact/test_term_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGfL-KnNm2BI"
   },
   "outputs": [],
   "source": [
    "# Apply data-preprocessing to keywords in most-depth\n",
    "df_train_md = pd.read_csv(\"/content/gdrive/My Drive/impact/train_most_depth.csv\")\n",
    "for index, row in df_train_md.iterrows():\n",
    "  value = call(row['abstract'])\n",
    "  value = lemmatize_sentence(value)\n",
    "  df_train_md.loc[index,'abstract'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAN5dobDm4Kw"
   },
   "outputs": [],
   "source": [
    "df_val_md = pd.read_csv(\"/content/gdrive/My Drive/impact/val_most_depth.csv\")\n",
    "for index, row in df_val_md.iterrows():\n",
    "  value = call(row['abstract'])\n",
    "  value = lemmatize_sentence(value)\n",
    "  df_val_md.loc[index,'abstract'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pcph3EJam58L"
   },
   "outputs": [],
   "source": [
    "df_test_md = pd.read_csv(\"/content/gdrive/My Drive/impact/test_most_depth.csv\")\n",
    "for index, row in df_test_md.iterrows():\n",
    "  value = call(row['abstract'])\n",
    "  value = lemmatize_sentence(value)\n",
    "  df_test_md.loc[index,'abstract'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQD39Enmm8Mn"
   },
   "outputs": [],
   "source": [
    "df_train_md.to_csv('/content/gdrive/My Drive/impact/train_md_preprocessed.csv', index=False)\n",
    "df_val_md.to_csv('/content/gdrive/My Drive/impact/val_md_preprocessed.csv', index=False)\n",
    "df_test_md.to_csv('/content/gdrive/My Drive/impact/test_md_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
